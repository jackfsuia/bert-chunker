{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First assemble a whole big dataset named `small_passenge.json`, using the [Nvidia dataset](https://huggingface.co/datasets/nvidia/ChatQA-Training-Data). That datasets include files like drop/train.json, narrativeqa/train.json, etc, which will be used below. Modify their file paths to your need.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "empty_df = pd.DataFrame()\n",
    "files=[\"ChatQA-Training-Data/drop/train.json\",\n",
    "       \"ChatQA-Training-Data/narrativeqa/train.json\",\n",
    "       \"ChatQA-Training-Data/quoref/train.json\",\n",
    "       \"ChatQA-Training-Data/newsqa/train.json\",\n",
    "       \"ChatQA-Training-Data/ropes/train.json\",\n",
    "       \"ChatQA-Training-Data/squad1.1/train.json\",\n",
    "       \"ChatQA-Training-Data/squad2.0/train.json\",\n",
    "       \"ChatQA-Training-Datat/synthetic_convqa/train.json\",\n",
    "       \"ChatQA-Training-Data/tatqa/train_arithmetic.json\",\n",
    "       \"ChatQA-Training-Data/tatqa/train_others.json\"]\n",
    "for file in files:\n",
    "    data = pd.read_json(file)\n",
    "    data_unique = data.drop_duplicates(subset=['document'])\n",
    "    data_unique=data_unique[['document']]\n",
    "    empty_df= pd.concat([empty_df, data_unique])\n",
    "empty_df = empty_df.drop_duplicates(subset=['document'])\n",
    "\n",
    "empty_df.to_json('small_passenge.json', orient='records', indent=4, date_format='iso')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate our chunking dataset `mixed_passenge_train_30000.json`. We sample from small_passenge.json 4 different rows to cross them into one row of chunking dataset `mixed_passenge_train_30000.json`, which they are separated by symbol \"<--start-->\". We generate 30000 of rows for chunking dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_SYM=\"<--start-->\"\n",
    "file=\"/data/full_data.jsonl\"\n",
    "import pandas as pd\n",
    "data = pd.read_json(file,lines=True)\n",
    "empty_df = pd.DataFrame()\n",
    "background_sentence_num=4\n",
    "avrge_tokens_per_sentence=20\n",
    "max_tokens=256\n",
    "\n",
    "import random\n",
    "# import nltk\n",
    "from tqdm import tqdm\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "for i in tqdm(range(0, 15000)):\n",
    "    sam = data.sample(n=1)\n",
    "    res = \"\"\n",
    "\n",
    "    for index, row in sam.iterrows():\n",
    "        allsen = row[\"story_zh\"].split('。')\n",
    "        allsen = [sents+'。' for sents in allsen if len(sents.strip())>0]\n",
    "        ranges = random.sample(range(len(allsen)+1), 2)\n",
    "        ranges.sort()\n",
    "        ss = allsen[ranges[0]:ranges[1]]\n",
    "        res = \"\".join(ss)\n",
    "        break\n",
    "    sam = pd.DataFrame({\"input\": [res]})\n",
    "    empty_df = pd.concat([empty_df, sam])\n",
    "\n",
    "for i in tqdm(range(0, 15000)):\n",
    "    sam = data.sample(n=2)\n",
    "    res = \"\"\n",
    "\n",
    "    for index, row in sam.iterrows():\n",
    "\n",
    "        sentence_num = random.randint(1, 5)\n",
    "        allsen = row[\"story_zh\"].split('。')\n",
    "        allsen = [sents+'。' for sents in allsen if len(sents.strip())>0]\n",
    "        ranges = random.sample(range(len(allsen)+1), 2)\n",
    "        ranges.sort()\n",
    "        ss = allsen[ranges[0]:ranges[1]]\n",
    "        ss = \"\".join(ss)\n",
    "        res = res + START_SYM + ss\n",
    "\n",
    "    res = res[11:]\n",
    "    sam = pd.DataFrame({\"input\": [res]})\n",
    "    empty_df = pd.concat([empty_df, sam])\n",
    "\n",
    "for i in tqdm(range(0, 15000)):\n",
    "    sam = data.sample(n=3)\n",
    "    res = \"\"\n",
    "\n",
    "    for index, row in sam.iterrows():\n",
    "\n",
    "        sentence_num = random.randint(1, 5)\n",
    "        allsen = row[\"story_zh\"].split('。')\n",
    "        allsen = [sents+'。' for sents in allsen if len(sents.strip())>0]\n",
    "        ranges = random.sample(range(len(allsen)+1), 2)\n",
    "        ranges.sort()\n",
    "        ss = allsen[ranges[0]:ranges[1]]\n",
    "        ss = \"\".join(ss)\n",
    "        res = res + START_SYM + ss\n",
    "\n",
    "    res = res[11:]\n",
    "    sam = pd.DataFrame({\"input\": [res]})\n",
    "    empty_df = pd.concat([empty_df, sam])\n",
    "    \n",
    "for i in tqdm(range(0, 15000)):\n",
    "    sam = data.sample(n=4)\n",
    "    res = \"\"\n",
    "\n",
    "    for index, row in sam.iterrows():\n",
    "\n",
    "        sentence_num = random.randint(1, 5)\n",
    "        allsen = row[\"story_zh\"].split('。')\n",
    "        allsen = [sents+'。' for sents in allsen if len(sents.strip())>0]\n",
    "        ranges = random.sample(range(len(allsen)+1), 2)\n",
    "        ranges.sort()\n",
    "        ss = allsen[ranges[0]:ranges[1]]\n",
    "        ss = \"\".join(ss)\n",
    "        res = res + START_SYM + ss\n",
    "\n",
    "    res = res[11:]\n",
    "    sam = pd.DataFrame({\"input\": [res]})\n",
    "    empty_df = pd.concat([empty_df, sam])\n",
    "    \n",
    "empty_df = empty_df.sample(frac=1).reset_index(drop=True)\n",
    "empty_df = empty_df.drop_duplicates(subset=[\"input\"])\n",
    "empty_df.to_json(\n",
    "    \"mixed_passenge_train.json\",\n",
    "    orient=\"records\",\n",
    "    indent=4,\n",
    "    date_format=\"iso\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same code but to generate `mixed_passenge_eval_30000.json` for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/data/BertChunker-zh/mixed_passenge_train.json','r',encoding='utf-8') as f:\n",
    "    mixed_passenge_train_40000 = json.loads(f.read())\n",
    "mixed_passenge_train_30000 = mixed_passenge_train_40000[:50000]\n",
    "mixed_passenge_test_10000 = mixed_passenge_train_40000[50000:]\n",
    "with open('/data/BertChunker-zh/mixed_passenge_train.json','w',encoding='utf-8') as f:\n",
    "    f.write(json.dumps(mixed_passenge_train_30000))\n",
    "with open('/data/BertChunker-zh/mixed_passenge_test.json','w',encoding='utf-8') as f:\n",
    "    f.write(json.dumps(mixed_passenge_test_10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/BertChunker-zh/mixed_passenge_test.json','r',encoding='utf-8') as f:\n",
    "    data=json.loads(f.read())\n",
    "print(data[100]['input'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
