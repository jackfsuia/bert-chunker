{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset\n",
    "START_SYM=\"<--start-->\"\n",
    "# use name=\"sample-10BT\" to use the 10BT sample\n",
    "# fw = load_dataset(\"/data/bert-chunker-v2/dataset/fineweb\", name=\"sample/10BT\", split=\"train\", streaming=True)\n",
    "# data_files = [\"000_00000.parquet\",\"001_00000.parquet\",\"002_00000.parquet\",]\n",
    "# data_files = [\"chinese-c4-0000-of-0096.jsonl\",\"chinese-c4-0001-of-0096.jsonl\",\"chinese-c4-0002-of-0096.jsonl\",\"chinese-c4-0003-of-0096.jsonl\",\"chinese-c4-0004-of-0096.jsonl\",\"chinese-c4-0005-of-0096.jsonl\",]\n",
    "\n",
    "# fw = load_dataset(\"/data/bert-chunker-v2/dataset/fineweb/sample/10BT\", data_files=data_files,split=\"train\",num_proc=6,cache_dir='/data/bert-chunker-v2/dataset/cache')\n",
    "# fw = load_dataset(\"/data/bert-chunker-v2/dataset/fineweb/sample/10BT\",split=\"train\",num_proc=6,cache_dir='/data/bert-chunker-v2/dataset/cache')\n",
    "fw = load_dataset(\"/data/bc-chinese-2/chinese-c4/data\",split=\"train\",num_proc=10,cache_dir='/data/bc-chinese-2/dataset/cache')\n",
    "\n",
    "print(fw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(fw[2]['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generating data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=10): 100%|██████████| 19273007/19273007 [00:52<00:00, 364085.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['url', 'timestamp', 'content_language', 'content_type', 'text'],\n",
      "    num_rows: 758527\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def fil_func(x):\n",
    "    def normalize_newlines(text):\n",
    "        import re\n",
    "        \n",
    "        return re.sub(r'\\n{2,}', '\\n', text.strip())\n",
    "    \n",
    "    \n",
    "    x_ =  normalize_newlines(x['text'])\n",
    "\n",
    "    p = len(x_)/x_.count('\\n')\n",
    "    if '\\n' in x_ and len(x_)>800 and START_SYM not in x_ and p>200 and len(x_)<2800:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "d = fw.filter(fil_func,num_proc=10)\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d[3330]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=10): 100%|██████████| 758527/758527 [00:24<00:00, 31450.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def cut(example):\n",
    "    \n",
    "    def normalize_newlines(text):\n",
    "        import re\n",
    "    \n",
    "        return re.sub(r'\\n{2,}', '\\n', text.strip())\n",
    "\n",
    "\n",
    "    txt =  normalize_newlines( example[\"text\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    START_SYM=\"<--start-->\"\n",
    "    example[\"text\"] =txt.replace('\\n',START_SYM)\n",
    "    return example\n",
    "\n",
    "# 使用 map 方法批量处理\n",
    "d = d.map(cut,num_proc=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.remove_columns(['url', 'timestamp', 'content_language', 'content_type', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d[200]['text'])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = d.select(range(0, 748527))\n",
    "d_test = d.select(range(748527, 758527))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is based on the revised code from fastchat based on tatsu-lab/stanford_alpaca.\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, Optional, List, Union\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import Trainer, GPTQConfig, AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from accelerate.utils import DistributedType\n",
    "from datasets import load_dataset\n",
    "import safetensors.torch\n",
    "import random\n",
    "\n",
    "\n",
    "IGNORE_TOKEN_ID = -100 \n",
    "START_SYM = \"<--start-->\"\n",
    "max_len = 512\n",
    "max_use_len = max_len-2\n",
    "\n",
    "def tokenize_function(item, tokenizer, max_len,):\n",
    "\n",
    "    chunks=re.split(START_SYM,item['text'])\n",
    "\n",
    "    # #加上，随机12%去掉句号\n",
    "    # precess_chunks=[]\n",
    "    # for c in chunks:\n",
    "    #     text = c.rstrip()        \n",
    "    #     chance = random.random()\n",
    "    #     if chance < 0.12 and text.endswith('.'):\n",
    "    #         text = text[:-1]+'\\n'\n",
    "    #         precess_chunks.append(text)\n",
    "    #     else:\n",
    "    #         precess_chunks.append(c)\n",
    "    # chunks = precess_chunks\n",
    "\n",
    "\n",
    "    # chunks=re.split(\"\\n\",item['text'])\n",
    "\n",
    "    input_ids=[]\n",
    "    labels=[]\n",
    "    for _,c in enumerate(chunks):\n",
    "        c=tokenizer(c,truncation=False)['input_ids']\n",
    "        #去掉头尾，最后两端再加上special tokens在头尾\n",
    "        c=c[1:-1]\n",
    "        if c:\n",
    "            input_ids+=c\n",
    "\n",
    "            cl=len(c)*[0]\n",
    "\n",
    "            cl[0]=1\n",
    "            labels+=cl\n",
    "    #判断数据是否是空，抛出异常\n",
    "    if not labels:\n",
    "        print('--------')\n",
    "        print(item)\n",
    "    labels[0]=0\n",
    "\n",
    "    full_texts = {\n",
    "            \"input_ids\": [],\n",
    "            \"labels\": [],\n",
    "            \"attention_mask\": [],\n",
    "        }\n",
    "        \n",
    "    start_idx = 0\n",
    "\n",
    "    while(1):\n",
    "\n",
    "        if start_idx>len(input_ids)-1:\n",
    "            break\n",
    "\n",
    "        end_idx = start_idx + max_use_len\n",
    "\n",
    "\n",
    "\n",
    "        window_input_ids = input_ids[start_idx:end_idx]\n",
    "        window_labels = labels[start_idx:end_idx]\n",
    "\n",
    "        # 加入bert 头尾\n",
    "        window_input_ids = [tokenizer.cls_token_id] + window_input_ids + [tokenizer.sep_token_id]\n",
    "\n",
    "        window_labels=[IGNORE_TOKEN_ID] + window_labels + [IGNORE_TOKEN_ID]\n",
    "\n",
    "        # 填充到max_len\n",
    "        len_now = len(window_input_ids)\n",
    "        window_input_ids = window_input_ids + [0] * (max_len - len_now)\n",
    "        window_labels = window_labels + [IGNORE_TOKEN_ID] * (max_len - len_now)\n",
    "        attention_mask = [1] * len_now + [0] * (max_len - len_now)\n",
    "\n",
    "        full_texts[\"input_ids\"].append(window_input_ids)\n",
    "        full_texts[\"labels\"].append(window_labels)\n",
    "        full_texts[\"attention_mask\"].append(attention_mask)\n",
    "\n",
    "\n",
    "        label_chunk = labels[start_idx:end_idx]\n",
    "        last_cut_id = end_idx\n",
    "        for i in range(len(label_chunk)):\n",
    "            j= len(label_chunk) - i - 1\n",
    "            if label_chunk[j]  == 1 and j !=0:\n",
    "                last_cut_id = start_idx + j\n",
    "                break\n",
    "\n",
    "        start_idx = last_cut_id\n",
    "\n",
    "    return full_texts\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/data/bc-chinese-2/bge-small-zh-v1.5\",\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=max_len,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "train_dataset = d_test.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=d_test.column_names,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"max_len\": max_len},\n",
    "    num_proc=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.to_pandas()\n",
    "train_dataset = train_dataset.explode(['input_ids',\"labels\",\"attention_mask\"],ignore_index=True)\n",
    "print(train_dataset)\n",
    "train_dataset.to_parquet(\"/data/bc-chinese-2/newline_10k.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
