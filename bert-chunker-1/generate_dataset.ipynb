{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First assemble a whole big dataset named `small_passenge.json`, using the [Nvidia dataset](https://huggingface.co/datasets/nvidia/ChatQA-Training-Data). That datasets include files like drop/train.json, narrativeqa/train.json, etc, which will be used below. Modify their file paths to your need.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "empty_df = pd.DataFrame()\n",
    "files=[\"ChatQA-Training-Data/drop/train.json\",\n",
    "       \"ChatQA-Training-Data/narrativeqa/train.json\",\n",
    "       \"ChatQA-Training-Data/quoref/train.json\",\n",
    "       \"ChatQA-Training-Data/newsqa/train.json\",\n",
    "       \"ChatQA-Training-Data/ropes/train.json\",\n",
    "       \"ChatQA-Training-Data/squad1.1/train.json\",\n",
    "       \"ChatQA-Training-Data/squad2.0/train.json\",\n",
    "       \"ChatQA-Training-Datat/synthetic_convqa/train.json\",\n",
    "       \"ChatQA-Training-Data/tatqa/train_arithmetic.json\",\n",
    "       \"ChatQA-Training-Data/tatqa/train_others.json\"]\n",
    "for file in files:\n",
    "    data = pd.read_json(file)\n",
    "    data_unique = data.drop_duplicates(subset=['document'])\n",
    "    data_unique=data_unique[['document']]\n",
    "    empty_df= pd.concat([empty_df, data_unique])\n",
    "empty_df = empty_df.drop_duplicates(subset=['document'])\n",
    "\n",
    "empty_df.to_json('small_passenge.json', orient='records', indent=4, date_format='iso')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate our chunking dataset `mixed_passenge_train_30000.json`. We sample from small_passenge.json 4 different rows to cross them into one row of chunking dataset `mixed_passenge_train_30000.json`, which they are separated by symbol \"<--start-->\". We generate 30000 of rows for chunking dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_SYM=\"<--start-->\"\n",
    "file=\"small_passenge.json\"\n",
    "import pandas as pd\n",
    "data = pd.read_json(file)\n",
    "empty_df = pd.DataFrame()\n",
    "background_sentence_num=4\n",
    "avrge_tokens_per_sentence=20\n",
    "max_tokens=256\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "for i in tqdm(range(0, 30000)):\n",
    "    sam = data.sample(n=4)\n",
    "    res = \"\"\n",
    "\n",
    "    for index, row in sam.iterrows():\n",
    "\n",
    "        sentence_num = random.randint(1, 5)\n",
    "        ss = sent_tokenize(row[\"document\"])[0:sentence_num]\n",
    "        ss = \"\".join(ss)\n",
    "        res = res + START_SYM + ss\n",
    "\n",
    "    res = res[11:]\n",
    "    sam = pd.DataFrame({\"input\": [res]})\n",
    "    empty_df = pd.concat([empty_df, sam])\n",
    "\n",
    "empty_df = empty_df.drop_duplicates(subset=[\"input\"])\n",
    "empty_df.to_json(\n",
    "    \"mixed_passenge_train_30000.json\",\n",
    "    orient=\"records\",\n",
    "    indent=4,\n",
    "    date_format=\"iso\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same code but to generate `mixed_passenge_eval_30000.json` for evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
