{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset\n",
    "START_SYM=\"<--start-->\"\n",
    "# use name=\"sample-10BT\" to use the 10BT sample\n",
    "# fw = load_dataset(\"/data/bert-chunker-v2/dataset/fineweb\", name=\"sample/10BT\", split=\"train\", streaming=True)\n",
    "data_files = [\"000_00000.parquet\",\"001_00000.parquet\",\"002_00000.parquet\",]\n",
    "# data_files = [\"014_00000.parquet\",]\n",
    "\n",
    "# fw = load_dataset(\"/data/bert-chunker-v2/dataset/fineweb/sample/10BT\", data_files=data_files,split=\"train\",num_proc=6,cache_dir='/data/bert-chunker-v2/dataset/cache')\n",
    "# fw = load_dataset(\"/data/bert-chunker-v2/dataset/fineweb/sample/10BT\",split=\"train\",num_proc=6,cache_dir='/data/bert-chunker-v2/dataset/cache')\n",
    "fw = load_dataset(\"/data/bert-chunker-v2/dataset/\", data_files=['/data/bert-chunker-v2/dataset/fw-1600k.parquet'],split=\"train\",num_proc=6,cache_dir='/data/bert-chunker-v2/dataset/cache')\n",
    "\n",
    "print(fw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset,Dataset\n",
    "START_SYM=\"<--start-->\"\n",
    "# use name=\"sample-10BT\" to use the 10BT sample\n",
    "# fw = load_dataset(\"/data/bert-chunker-v2/dataset/fineweb\", name=\"sample/10BT\", split=\"train\", streaming=True)\n",
    "data_files = [\"000_00000.parquet\",\"001_00000.parquet\",\"002_00000.parquet\",]\n",
    "# data_files = [\"014_00000.parquet\",]\n",
    "\n",
    "# fw = load_dataset(\"/data/bert-chunker-v2/dataset/fineweb/sample/10BT\", data_files=data_files,split=\"train\",num_proc=6,cache_dir='/data/bert-chunker-v2/dataset/cache')\n",
    "# fw = load_dataset(\"/data/bert-chunker-v2/dataset/fineweb/sample/10BT\",split=\"train\",num_proc=6,cache_dir='/data/bert-chunker-v2/dataset/cache')\n",
    "fw = load_dataset(\"/data/bert-chunker-v2/dataset/\", data_files=['/data/bert-chunker-v2/dataset/fw-train-cross.parquet'],split=\"train\",num_proc=6,cache_dir='/data/bert-chunker-v2/dataset/cache')\n",
    "\n",
    "print(fw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = fw.filter(lambda x: 'microcomputer' in x['input'],num_proc=6)\n",
    "print(d)\n",
    "print(d[2]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=6): 100%|██████████| 3143000/3143000 [00:10<00:00, 311119.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# d = fw.filter(lambda x: x[\"text\"].count('\\n')>=2 and x['token_count']<=150,num_proc=6)\n",
    "# d = fw.filter(lambda x:  x['token_count']<=150 and x['token_count']>=50,num_proc=6)\n",
    "d = fw.filter(lambda x:  x['token_count']>250,num_proc=6)\n",
    "\n",
    "# def cut(example):\n",
    "#     example[\"input\"] = example['text']\n",
    "#     return example\n",
    "\n",
    "# # 使用 map 方法批量处理\n",
    "# d = d.map(cut,num_proc=6)\n",
    "# last_100_elements = d[0:10000]\n",
    "\n",
    "# # 形成新的数据集\n",
    "# d = Dataset.from_dict(last_100_elements)\n",
    "# d.to_parquet(\"/data/bert-chunker-v2/dataset/fw-train-onepara.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],\n",
      "    num_rows: 29303\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d[2]['text'])\n",
    "\n",
    "print(d[2]['token_count'])\n",
    "d = d.shuffle(seed=42)\n",
    "print(d[2]['text'])\n",
    "\n",
    "print(d[2]['token_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],\n",
      "    num_rows: 40651\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=6): 100%|██████████| 40651/40651 [00:01<00:00, 22457.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def cut(example):\n",
    "    START_SYM=\"<--start-->\"\n",
    "    example[\"input\"] = example['text']+START_SYM\n",
    "    return example\n",
    "\n",
    "# 使用 map 方法批量处理\n",
    "d = d.map(cut,num_proc=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.remove_columns(['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=6):   0%|          | 0/40651 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=6): 100%|██████████| 40651/40651 [00:00<00:00, 223073.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def merge_batch(batch, group_size=3):\n",
    "    START_SYM=\"<--start-->\"\n",
    "    new_data = {\"input\": [],}\n",
    "    for i in range(0, len(batch['input']), group_size):\n",
    "        # 获取当前组的 4 条数据\n",
    "        texts = batch['input'][i:i+group_size]\n",
    "        \n",
    "        # 合并数据\n",
    "        merged_text = ' '.join(texts)\n",
    "        merged_text = merged_text[:-len(START_SYM)]\n",
    "        \n",
    "        # 添加到新数据中\n",
    "        new_data[\"input\"].append(merged_text)\n",
    "    return new_data\n",
    "\n",
    "# 使用 Dataset.map 处理数据\n",
    "new_dataset = d.map(\n",
    "    merge_batch,\n",
    "    batched=True,  # 启用批处理\n",
    "    batch_size=1000,  # 每批次处理 1000 条原始数据\n",
    "    remove_columns=d.column_names,  # 移除原始列\n",
    "    num_proc=6,  # 使用 6 个进程并行处理\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取最后 100 个元素\n",
    "last_100_elements = new_dataset[-10000:]\n",
    "\n",
    "# 形成新的数据集\n",
    "d = Dataset.from_dict(last_100_elements)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 101.35ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14685418"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.to_parquet(\"/data/bert-chunker-v2/dataset/fw-test-cross.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
