{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset\n",
    "START_SYM=\"<--start-->\"\n",
    "\n",
    "data_files = [\"000_00000.parquet\",\"001_00000.parquet\",\"002_00000.parquet\",]\n",
    "\n",
    "fw = load_dataset(\"/data/bert-chunker-v2/dataset/fineweb/sample/10BT\", data_files=data_files,split=\"train\",num_proc=6,cache_dir='/data/bert-chunker-v2/dataset/cache')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=6): 100%|██████████| 3143000/3143000 [00:11<00:00, 278468.14 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],\n",
      "    num_rows: 1405557\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fw2=fw.filter(lambda x: x['token_count']>50 and x['language_score']>0.95,num_proc=6)\n",
    "print(fw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fw2[2001]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw2 = fw2.select(range(2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据合成 generating dataset："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, Optional, List, Union\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import Trainer, GPTQConfig, AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from accelerate.utils import DistributedType\n",
    "from datasets import load_dataset\n",
    "import safetensors.torch\n",
    "\n",
    "\n",
    "IGNORE_TOKEN_ID = -100 \n",
    "START_SYM = \"<--start-->\"\n",
    "\n",
    "\n",
    "def tokenize_function(item, tokenizer, max_len, stride):\n",
    "\n",
    "    # chunks=re.split(START_SYM,item['input'])\n",
    "    chunks=re.split(\"\\n\",item['text'])\n",
    "\n",
    "    input_ids=[]\n",
    "    labels=[]\n",
    "    for _,c in enumerate(chunks):\n",
    "        c=tokenizer(c,truncation=False)['input_ids']\n",
    "        #去掉头尾，最后两端再加上special tokens在头尾\n",
    "        c=c[1:-1]\n",
    "        if c:\n",
    "            input_ids+=c\n",
    "\n",
    "            cl=len(c)*[0]\n",
    "\n",
    "            cl[0]=1\n",
    "            labels+=cl\n",
    "    #判断数据是否是空，抛出异常\n",
    "    if not labels:\n",
    "        print('--------')\n",
    "        print(item)\n",
    "    labels[0]=0\n",
    "\n",
    "    full_texts = {\n",
    "            \"input_ids\": [],\n",
    "            \"labels\": [],\n",
    "            \"attention_mask\": [],\n",
    "        }\n",
    "    for start_idx in range(0, len(input_ids), stride):\n",
    "        end_idx = start_idx + max_len  - 2\n",
    "\n",
    "        if end_idx>len(input_ids) +50 and full_texts[\"input_ids\"]:\n",
    "            break\n",
    "\n",
    "        window_input_ids = input_ids[start_idx:end_idx]\n",
    "        window_labels = labels[start_idx:end_idx]\n",
    "\n",
    "        # 加入bert 头尾\n",
    "        window_input_ids = [tokenizer.cls_token_id]+window_input_ids+[tokenizer.sep_token_id]\n",
    "\n",
    "        window_labels=[IGNORE_TOKEN_ID]+window_labels+[IGNORE_TOKEN_ID]\n",
    "\n",
    "        # 填充到max_len\n",
    "        len_now = len(window_input_ids)\n",
    "        window_input_ids = window_input_ids + [0] * (max_len - len_now)\n",
    "        window_labels = window_labels + [IGNORE_TOKEN_ID] * (max_len - len_now)\n",
    "        attention_mask = [1] * len_now + [0] * (max_len - len_now)\n",
    "\n",
    "        full_texts[\"input_ids\"].append(window_input_ids)\n",
    "        full_texts[\"labels\"].append(window_labels)\n",
    "        full_texts[\"attention_mask\"].append(attention_mask)\n",
    "\n",
    "\n",
    "    return full_texts\n",
    "\n",
    "model_max_length = 255\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/data/all-MiniLM-L6-v2\",\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=model_max_length,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = fw2.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=fw.column_names,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"max_len\": model_max_length, \"stride\": 50},\n",
    "    num_proc=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels', 'attention_mask'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.explode(['input_ids',\"labels\",\"attention_mask\"],ignore_index=True)\n",
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_parquet(\"/data/bert-chunker-v2/dataset/fwtrain-slidewindow.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
