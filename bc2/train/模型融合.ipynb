{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First assemble a whole big dataset named `small_passenge.json`, using the [Nvidia dataset](https://huggingface.co/datasets/nvidia/ChatQA-Training-Data). That datasets include files like drop/train.json, narrativeqa/train.json, etc, which will be used below. Modify their file paths to your need.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "empty_df = pd.DataFrame()\n",
    "files=[\"ChatQA-Training-Data/drop/train.json\",\n",
    "       \"ChatQA-Training-Data/narrativeqa/train.json\",\n",
    "       \"ChatQA-Training-Data/quoref/train.json\",\n",
    "       \"ChatQA-Training-Data/newsqa/train.json\",\n",
    "       \"ChatQA-Training-Data/ropes/train.json\",\n",
    "       \"ChatQA-Training-Data/squad1.1/train.json\",\n",
    "       \"ChatQA-Training-Data/squad2.0/train.json\",\n",
    "       \"ChatQA-Training-Datat/synthetic_convqa/train.json\",\n",
    "       \"ChatQA-Training-Data/tatqa/train_arithmetic.json\",\n",
    "       \"ChatQA-Training-Data/tatqa/train_others.json\"]\n",
    "for file in files:\n",
    "    data = pd.read_json(file)\n",
    "    data_unique = data.drop_duplicates(subset=['document'])\n",
    "    data_unique=data_unique[['document']]\n",
    "    empty_df= pd.concat([empty_df, data_unique])\n",
    "empty_df = empty_df.drop_duplicates(subset=['document'])\n",
    "\n",
    "empty_df.to_json('small_passenge.json', orient='records', indent=4, date_format='iso')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate our chunking dataset `mixed_passenge_train_30000.json`. We sample from small_passenge.json 4 different rows to cross them into one row of chunking dataset `mixed_passenge_train_30000.json`, which they are separated by symbol \"<--start-->\". We generate 30000 of rows for chunking dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# nltk.download('punkt')\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpunkt_tab\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m30000\u001b[39m)):\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/site-packages/nltk/downloader.py:774\u001b[0m, in \u001b[0;36mDownloader.download\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(s, prefix2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    766\u001b[0m     print_to(\n\u001b[1;32m    767\u001b[0m         textwrap\u001b[38;5;241m.\u001b[39mfill(\n\u001b[1;32m    768\u001b[0m             s,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    771\u001b[0m         )\n\u001b[1;32m    772\u001b[0m     )\n\u001b[0;32m--> 774\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincr_download(info_or_id, download_dir, force):\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;66;03m# Error messages\u001b[39;00m\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(msg, ErrorMessage):\n\u001b[1;32m    777\u001b[0m         show(msg\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/site-packages/nltk/downloader.py:629\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m# Look up the requested collection or package.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_or_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_or_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m ErrorMessage(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo_or_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/site-packages/nltk/downloader.py:603\u001b[0m, in \u001b[0;36mDownloader._info_or_id\u001b[0;34m(self, info_or_id)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_info_or_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, info_or_id):\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(info_or_id, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_or_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m info_or_id\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/site-packages/nltk/downloader.py:1006\u001b[0m, in \u001b[0;36mDownloader.info\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfo\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mid\u001b[39m):\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the ``Package`` or ``Collection`` record for the\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03m    given item.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packages:\n\u001b[1;32m   1008\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packages[\u001b[38;5;28mid\u001b[39m]\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/site-packages/nltk/downloader.py:949\u001b[0m, in \u001b[0;36mDownloader._update_index\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url \u001b[38;5;241m=\u001b[39m url \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# Download the index file.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39minternals\u001b[38;5;241m.\u001b[39mElementWrapper(\n\u001b[0;32m--> 949\u001b[0m     \u001b[43mElementTree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgetroot()\n\u001b[1;32m    950\u001b[0m )\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_timestamp \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# Build a dictionary of packages.\u001b[39;00m\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/xml/etree/ElementTree.py:1222\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse XML document into element tree.\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \n\u001b[1;32m   1215\u001b[0m \u001b[38;5;124;03m*source* is a filename or file object containing XML data,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m tree \u001b[38;5;241m=\u001b[39m ElementTree()\n\u001b[0;32m-> 1222\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/xml/etree/ElementTree.py:580\u001b[0m, in \u001b[0;36mElementTree.parse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    574\u001b[0m     parser \u001b[38;5;241m=\u001b[39m XMLParser()\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(parser, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parse_whole\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;66;03m# The default XMLParser, when it comes from an accelerator,\u001b[39;00m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;66;03m# can define an internal _parse_whole API for efficiency.\u001b[39;00m\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;66;03m# It can be used to parse the whole source without feeding\u001b[39;00m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;66;03m# it with chunks.\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "START_SYM=\"<--start-->\"\n",
    "file=\"/data/bert-chunker-v2/small_passenge.json\"\n",
    "import pandas as pd\n",
    "data = pd.read_json(file)\n",
    "empty_df = pd.DataFrame()\n",
    "background_sentence_num=4\n",
    "avrge_tokens_per_sentence=20\n",
    "max_tokens=256\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "# nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "for i in tqdm(range(0, 30000)):\n",
    "    sam = data.sample(n=4)\n",
    "    res = \"\"\n",
    "\n",
    "    for index, row in sam.iterrows():\n",
    "\n",
    "        sentence_num = random.randint(1, 5)\n",
    "        ss = sent_tokenize(row[\"document\"])[0:sentence_num]\n",
    "        ss = \"\".join(ss)\n",
    "        res = res + START_SYM + ss\n",
    "\n",
    "    res = res[11:]\n",
    "    sam = pd.DataFrame({\"input\": [res]})\n",
    "    empty_df = pd.concat([empty_df, sam])\n",
    "\n",
    "empty_df = empty_df.drop_duplicates(subset=[\"input\"])\n",
    "empty_df.to_json(\n",
    "    \"/data/mixed_passenge_train_30000.json\",\n",
    "    orient=\"records\",\n",
    "    indent=4,\n",
    "    date_format=\"iso\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same code but to generate `mixed_passenge_eval_30000.json` for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是临时的模型融合代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "# 加载两个模型的权重\n",
    "weights1 = load_file(\"/data/bert-chunker-v2/model_soup/huanhang-model.safetensors\")\n",
    "weights2 = load_file(\"/data/bert-chunker-v2/model_soup/yuyi-model.safetensors\")\n",
    "weights2 = load_file(\"/data/tim1900/bert-chunker/model.safetensors\")\n",
    "\n",
    "\n",
    "# 定义权重\n",
    "w1, w2 = 0.9,0.1  # 可以根据需要调整\n",
    "\n",
    "# 创建一个字典来保存新的加权平均权重\n",
    "new_weights = {}\n",
    "\n",
    "# 遍历所有参数并计算加权平均\n",
    "for key in weights1.keys():\n",
    "    if key in weights2:  # 确认两个模型都有这个参数\n",
    "        new_weights[key] = (w1 * weights1[key] + w2 * weights2[key]) / (w1 + w2)\n",
    "\n",
    "# 保存新的权重到一个新的 .safetensors 文件\n",
    "save_file(new_weights, \"/data/bert-chunker-v2/model_soup/model.safetensors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
